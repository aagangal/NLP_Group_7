{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb8b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from gensim.matutils import hellinger\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573ac7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f78c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\trevo\\NLP\\data\\ReducedDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e03e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139419</td>\n",
       "      <td>/foo-fighters/</td>\n",
       "      <td>Hey, Johnny Park!</td>\n",
       "      <td>/foo-fighters/hey-johnny-park.html</td>\n",
       "      <td>Come and I'll take you under\\nThis beautiful b...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290738</td>\n",
       "      <td>/mxpx/</td>\n",
       "      <td>Call In Sick</td>\n",
       "      <td>/mxpx/call-in-sick.html</td>\n",
       "      <td>Oh how I missed you,\\nOh how I needed you toda...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162905</td>\n",
       "      <td>/arch-enemy/</td>\n",
       "      <td>Despicable Heroes</td>\n",
       "      <td>/arch-enemy/despicable-heroes.html</td>\n",
       "      <td>I spit in your face, preacers and leaders\\nSpe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281035</td>\n",
       "      <td>/the-maine/</td>\n",
       "      <td>Whoever She Is</td>\n",
       "      <td>/the-maine/whoever-she-is.html</td>\n",
       "      <td>I thought I had my girl but she ran away\\nMy c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253213</td>\n",
       "      <td>/a-ha/</td>\n",
       "      <td>Days On End</td>\n",
       "      <td>/a-ha/days-on-end.html</td>\n",
       "      <td>Do know why winter's such a cold and lonely pl...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           ALink              SName  \\\n",
       "0      139419  /foo-fighters/  Hey, Johnny Park!   \n",
       "1      290738          /mxpx/       Call In Sick   \n",
       "2      162905    /arch-enemy/  Despicable Heroes   \n",
       "3      281035     /the-maine/     Whoever She Is   \n",
       "4      253213          /a-ha/        Days On End   \n",
       "\n",
       "                                SLink  \\\n",
       "0  /foo-fighters/hey-johnny-park.html   \n",
       "1             /mxpx/call-in-sick.html   \n",
       "2  /arch-enemy/despicable-heroes.html   \n",
       "3      /the-maine/whoever-she-is.html   \n",
       "4              /a-ha/days-on-end.html   \n",
       "\n",
       "                                               Lyric language  \n",
       "0  Come and I'll take you under\\nThis beautiful b...       en  \n",
       "1  Oh how I missed you,\\nOh how I needed you toda...       en  \n",
       "2  I spit in your face, preacers and leaders\\nSpe...       en  \n",
       "3  I thought I had my girl but she ran away\\nMy c...       en  \n",
       "4  Do know why winter's such a cold and lonely pl...       en  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab431bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df[df['language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d078b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139419</td>\n",
       "      <td>/foo-fighters/</td>\n",
       "      <td>Hey, Johnny Park!</td>\n",
       "      <td>/foo-fighters/hey-johnny-park.html</td>\n",
       "      <td>Come and I'll take you under\\nThis beautiful b...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290738</td>\n",
       "      <td>/mxpx/</td>\n",
       "      <td>Call In Sick</td>\n",
       "      <td>/mxpx/call-in-sick.html</td>\n",
       "      <td>Oh how I missed you,\\nOh how I needed you toda...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162905</td>\n",
       "      <td>/arch-enemy/</td>\n",
       "      <td>Despicable Heroes</td>\n",
       "      <td>/arch-enemy/despicable-heroes.html</td>\n",
       "      <td>I spit in your face, preacers and leaders\\nSpe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281035</td>\n",
       "      <td>/the-maine/</td>\n",
       "      <td>Whoever She Is</td>\n",
       "      <td>/the-maine/whoever-she-is.html</td>\n",
       "      <td>I thought I had my girl but she ran away\\nMy c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253213</td>\n",
       "      <td>/a-ha/</td>\n",
       "      <td>Days On End</td>\n",
       "      <td>/a-ha/days-on-end.html</td>\n",
       "      <td>Do know why winter's such a cold and lonely pl...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           ALink              SName  \\\n",
       "0      139419  /foo-fighters/  Hey, Johnny Park!   \n",
       "1      290738          /mxpx/       Call In Sick   \n",
       "2      162905    /arch-enemy/  Despicable Heroes   \n",
       "3      281035     /the-maine/     Whoever She Is   \n",
       "4      253213          /a-ha/        Days On End   \n",
       "\n",
       "                                SLink  \\\n",
       "0  /foo-fighters/hey-johnny-park.html   \n",
       "1             /mxpx/call-in-sick.html   \n",
       "2  /arch-enemy/despicable-heroes.html   \n",
       "3      /the-maine/whoever-she-is.html   \n",
       "4              /a-ha/days-on-end.html   \n",
       "\n",
       "                                               Lyric language  \n",
       "0  Come and I'll take you under\\nThis beautiful b...       en  \n",
       "1  Oh how I missed you,\\nOh how I needed you toda...       en  \n",
       "2  I spit in your face, preacers and leaders\\nSpe...       en  \n",
       "3  I thought I had my girl but she ran away\\nMy c...       en  \n",
       "4  Do know why winter's such a cold and lonely pl...       en  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085e9f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38363"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74b6357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm comin' home, I've done my time\n",
      "Now I've got to know what is and isn't mine\n",
      "If you received my letter telling you I'd soon be free\n",
      "Then you'll know just what to do\n",
      "If you still want me\n",
      "If you still want me\n",
      "Whoa, tie a yellow ribbon 'round the old oak tree\n",
      "It's been three long years\n",
      "Do ya still want me?\n",
      "If I don't see a ribbon round the old oak tree\n",
      "I'll stay on the bus\n",
      "Forget about us\n",
      "Put the blame on me\n",
      "If I don't see a yellow ribbon round the old oak tree\n",
      "\n",
      "Bus driver, please look for me\n",
      "'cause I couldn't bear to see what I might see\n",
      "I'm really still in prison\n",
      "And my love, she holds the key\n",
      "A simple yellow ribbon's what I need to set me free\n",
      "I wrote and told her please\n",
      "\n",
      "Whoa, tie a yellow ribbon round the old oak tree\n",
      "It's been three long years\n",
      "Do ya still want me?\n",
      "If I don't see a ribbon round the old oak tree\n",
      "I'll stay on the bus\n",
      "Forget about us\n",
      "Put the blame on me\n",
      "If I don't see a yellow ribbon round the old oak tree\n",
      "\n",
      "Now the whole damned bus is cheerin'\n",
      "And I can't believe I see\n",
      "A hundred yellow ribbons round the old oak tree\n"
     ]
    }
   ],
   "source": [
    "print(df_en['Lyric'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ba571b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords library\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Establish a word punctuation tokenizer\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "# Establish the English stop words\n",
    "basic_stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "custom_stop_words = ['get', 'yeah', 's', 'ai', 'ca', 'like', 'nt', 'ta', 'oh', 'got', 'gonna','goin','na','I', \"i'm\", \"ain't\", 'come', 'make', 'know', 'gotta']\n",
    "\n",
    "stop_words = basic_stop_words + custom_stop_words\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # Lowercase and remove special characters and whitespaces\n",
    "    doc = re.sub(r\"[^a-zA-Z\\s']\", '', doc, re.I | re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "\n",
    "    # Tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and not token.isdigit()]\n",
    "    # Re-create the document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "\n",
    "        # Remove punctuation\n",
    "    doc = re.sub(f\"[{re.escape(string.punctuation)}]\", '', doc)\n",
    "\n",
    "    doc = re.sub(r\"'\\s*\", \"\", doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f5ea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(df_en['Lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a35cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " take beautiful bruise  colors everything fades time  true wish another stab undercover change mind  impossible  let  never selling sit watch every mood eyes still remind angels hover eyes change blind blue  impossible  let  never selling sit watch every mood  found reward  throw away long  share piece mine  impossible  let  never selling sit watch every mood\n"
     ]
    }
   ],
   "source": [
    "print(norm_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a046be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "take take\n",
      "beautiful beautiful\n",
      "bruise bruise\n",
      "   \n",
      "colors color\n",
      "everything everything\n",
      "fades fade\n",
      "time time\n",
      "   \n",
      "true true\n",
      "wish wish\n",
      "another another\n",
      "stab stab\n",
      "undercover undercover\n",
      "change change\n",
      "mind mind\n",
      "   \n",
      "impossible impossible\n",
      "   \n",
      "let let\n",
      "   \n",
      "never never\n",
      "selling sell\n",
      "sit sit\n",
      "watch watch\n",
      "every every\n",
      "mood mood\n",
      "eyes eye\n",
      "still still\n",
      "remind remind\n",
      "angels angel\n",
      "hover hover\n",
      "eyes eye\n",
      "change change\n",
      "blind blind\n",
      "blue blue\n",
      "   \n",
      "impossible impossible\n",
      "   \n",
      "let let\n",
      "   \n",
      "never never\n",
      "selling sell\n",
      "sit sit\n",
      "watch watch\n",
      "every every\n",
      "mood mood\n",
      "   \n",
      "found find\n",
      "reward reward\n",
      "   \n",
      "throw throw\n",
      "away away\n",
      "long long\n",
      "   \n",
      "share share\n",
      "piece piece\n",
      "mine mine\n",
      "   \n",
      "impossible impossible\n",
      "   \n",
      "let let\n",
      "   \n",
      "never never\n",
      "selling sell\n",
      "sit sit\n",
      "watch watch\n",
      "every every\n",
      "mood mood\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(norm_corpus[0].item())\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c25f1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = []\n",
    "\n",
    "for text in norm_corpus:\n",
    "    doc = nlp(text.item())  \n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatized_corpus.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ecfbc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  take beautiful bruise   color everything fade time   true wish another stab undercover change mind   impossible   let   never sell sit watch every mood eye still remind angel hover eye change blind blue   impossible   let   never sell sit watch every mood   find reward   throw away long   share piece mine   impossible   let   never sell sit watch every mood\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lemmatized_corpus, columns=['lyrics'])\n",
    "df.to_csv('output_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1af1904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [text.split() for text in lemmatized_corpus]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(202, 64544), (246, 54282), (209, 40490), (67, 37575), (32, 34549), (104, 33670), (16, 32458), (30, 31321), (282, 30233), (21, 29150)]\n",
      "Word: love, Frequency: 64544\n",
      "Word: go, Frequency: 54282\n",
      "Word: say, Frequency: 40490\n",
      "Word: see, Frequency: 37575\n",
      "Word: time, Frequency: 34549\n",
      "Word: one, Frequency: 33670\n",
      "Word: let, Frequency: 32458\n",
      "Word: take, Frequency: 31321\n",
      "Word: want, Frequency: 30233\n",
      "Word: never, Frequency: 29150\n"
     ]
    }
   ],
   "source": [
    "# check the work frequency within our dictionary that we will be using for the lda model, if a word is too frequenct it may lose value\n",
    "# Too frequent words have been added as stop words on each iteration.\n",
    "top_10_most_frequent_words = sorted(dictionary.cfs.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "print(top_10_most_frequent_words)\n",
    "for token_id, frequency in top_10_most_frequent_words:\n",
    "    word = dictionary[token_id]\n",
    "    print(f\"Word: {word}, Frequency: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=35, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.119*\"la\" + 0.021*\"boom\" + 0.020*\"joan\" + 0.019*\"baez\" + 0.019*\"beautiful\" + 0.018*\"bye\" + 0.017*\"greedy\" + 0.017*\"hello\" + 0.017*\"de\" + 0.015*\"e\"\n",
      "Topic 1: 0.060*\"baby\" + 0.041*\"girl\" + 0.030*\"wanna\" + 0.023*\"want\" + 0.021*\"say\" + 0.020*\"hey\" + 0.019*\"love\" + 0.014*\"ooh\" + 0.014*\"boy\" + 0.013*\"right\"\n",
      "Topic 2: 0.045*\"love\" + 0.021*\"go\" + 0.019*\"time\" + 0.018*\"never\" + 0.017*\"say\" + 0.015*\"one\" + 0.014*\"feel\" + 0.014*\"see\" + 0.012*\"heart\" + 0.012*\"want\"\n",
      "Topic 3: 0.111*\"lyric\" + 0.063*\"bill\" + 0.053*\"monroe\" + 0.038*\"blue\" + 0.020*\"woah\" + 0.019*\"lyrics\" + 0.016*\"new\" + 0.011*\"city\" + 0.010*\"train\" + 0.010*\"john\"\n",
      "Topic 4: 0.021*\"man\" + 0.017*\"well\" + 0.016*\"go\" + 0.014*\"one\" + 0.014*\"say\" + 0.011*\"run\" + 0.010*\"people\" + 0.009*\"old\" + 0.009*\"look\" + 0.007*\"see\"\n",
      "Topic 5: 0.014*\"nigga\" + 0.014*\"shit\" + 0.013*\"fuck\" + 0.012*\"bitch\" + 0.011*\"ya\" + 0.009*\"they\" + 0.009*\"niggas\" + 0.009*\"yo\" + 0.008*\"money\" + 0.008*\"back\"\n",
      "Topic 6: 0.018*\"life\" + 0.013*\"take\" + 0.012*\"feel\" + 0.011*\"die\" + 0.010*\"lie\" + 0.009*\"live\" + 0.009*\"break\" + 0.008*\"lose\" + 0.008*\"mind\" + 0.008*\"burn\"\n",
      "Topic 7: 0.074*\"let\" + 0.059*\"go\" + 0.029*\"tonight\" + 0.026*\"dance\" + 0.023*\"night\" + 0.019*\"rock\" + 0.016*\"move\" + 0.015*\"party\" + 0.014*\"everybody\" + 0.013*\"stop\"\n",
      "Topic 8: 0.012*\"light\" + 0.011*\"night\" + 0.011*\"see\" + 0.010*\"dream\" + 0.009*\"sky\" + 0.009*\"eye\" + 0.009*\"sun\" + 0.008*\"god\" + 0.008*\"rain\" + 0.008*\"we\"\n",
      "Topic 9: 0.110*\"nt\" + 0.071*\"I\" + 0.065*\"m\" + 0.063*\"do\" + 0.041*\"s\" + 0.036*\"da\" + 0.031*\"you\" + 0.028*\"u\" + 0.025*\"be\" + 0.025*\"nah\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id in range(10):\n",
    "    topic = lda_model.print_topic(topic_id)\n",
    "    print(f\"Topic {topic_id}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word  Count\n",
      "14      say      3\n",
      "25      see      3\n",
      "20       go      3\n",
      "24     feel      2\n",
      "23      one      2\n",
      "65    night      2\n",
      "16     love      2\n",
      "13     want      2\n",
      "56      lie      1\n",
      "63  tonight      1\n"
     ]
    }
   ],
   "source": [
    "# Verify that a singular word is not used in a large majority of the topics or else that word may not hold much value\n",
    "word_counts = {}\n",
    "num_topics = lda_model.num_topics\n",
    "\n",
    "for topic_id in range(num_topics):\n",
    "    topic_words = lda_model.show_topic(topic_id, topn=10) \n",
    "\n",
    "    for word, prob in topic_words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "word_count_topic_list = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n",
    "\n",
    "word_count_topic_list = word_count_topic_list.sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(word_count_topic_list.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "800298a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_topic_distribution = [lda_model[doc] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 most similar songs to the input song Star by /beck/:\n",
      "Song See Water by /beck/\n",
      "Song Sober And Unkissed by /sia/\n",
      "Song Sausalito by /bright-eyes/\n",
      "Song The end by /alphaville/\n",
      "Song No Snow On The Mountain by /nada-surf/\n"
     ]
    }
   ],
   "source": [
    "def calculate_hellinger_distance(song_dist_1, song_dist_2):\n",
    "    return hellinger(song_dist_1, song_dist_2)\n",
    "\n",
    "input_song_index = 139\n",
    "\n",
    "input_song_dist = song_topic_distribution[input_song_index]\n",
    "\n",
    "hellinger_distances = []\n",
    "\n",
    "for i, song_dist in enumerate(song_topic_distribution):\n",
    "\n",
    "    if i != input_song_index:\n",
    "        \n",
    "        distance = calculate_hellinger_distance(input_song_dist, song_dist)\n",
    "        hellinger_distances.append((distance, i))\n",
    "\n",
    "hellinger_distances.sort(key=lambda x: x[0])\n",
    "\n",
    "top_5_similar_songs = hellinger_distances[:5]\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTop 5 most similar songs to the input song {df_en['SName'].iloc[input_song_index]} by {df_en['ALink'].iloc[input_song_index]}:\")\n",
    "for distance, song_index in top_5_similar_songs:\n",
    "    print(f\"Song {df_en['SName'].iloc[song_index]} by {df_en['ALink'].iloc[song_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.43475226),\n",
       " (3, 0.28249836),\n",
       " (11, 0.12165525),\n",
       " (13, 0.07855593),\n",
       " (14, 0.07209814)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_topic_distribution[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So Much Pain\n",
      "[Master P and (Mo B. Dick)]\n",
      "Oh like that (Ooh)\n",
      "Tre-8 they ain't ready for this dog (Ooh)\n",
      "Smoke One and No Limit (Ooh)\n",
      "All the way from California to New Orleans\n",
      "Ask em' about it, so much pain boy (So much pain)\n",
      "\n",
      "[Master P]\n",
      "Birds in the kitchen, palms itchin'\n",
      "And all y'all niggas in the game pay attention\n",
      "As I teach, ain't got no time to preach\n",
      "2 for 3, 4 for 5, 16 a fuckin' key\n",
      "Don't laugh, niggas like to backstab\n",
      "But where I'm from see yo brother on a body slab\n",
      "New Orleans, the city of the candy cream\n",
      "A bunch of projects full of jackers and dope fiends\n",
      "As I cry, think one day I gotta die\n",
      "But I don't give a fuck cause ain't no love from the outside\n",
      "As I walk to the projects\n",
      "Niggas killin' dope fiends behind fuckin' county checks\n",
      "And my younger homie smokin' dope\n",
      "The niggas I used to hang with doin' that boy broke\n",
      "And they gone off that water, water\n",
      "Ain't no love from New Orleans all the way back to Florida\n",
      "It's just a bunch of pain\n",
      "\n",
      "[Chorus: Mo B. Dick]\n",
      "So much pain, so much pain\n",
      "So much pain, so much pain\n",
      "\n",
      "[Mia X]\n",
      "I'm feelin' so much pain, holdin' back my tears of anger\n",
      "As I walk through this danger zone\n",
      "That I used to call home\n",
      "Strangers got my nerves all jumpy\n",
      "But it's best I watch them niggas who be smilin' actin' chummy\n",
      "Like it's all real\n",
      "I don't know why they got it in for me\n",
      "Like my girl Jill, who was sleepin' with the enemy\n",
      "Never knew the handy dope was a sad one\n",
      "Now I know the vibes that I felt was some bad ones\n",
      "Takes a mad one, females so precious\n",
      "Tryin' to move on since our best friend left us\n",
      "Got my baby still stuck in this drama filled blue\n",
      "Tryin' to get my pockets on so I can snatch him from this zoo\n",
      "And paint a clearer picture cause the one I see now\n",
      "Got my vision all blurry, hard to see my way out\n",
      "Bow my head to Lord and pray hard for a change\n",
      "Can lift a sister from feelin' this\n",
      "So much pain, so much pain\n",
      "\n",
      "[Chorus]\n",
      "\n",
      "[Tre-8]\n",
      "Growin' up as a youngster, comin' around that thug life\n",
      "Dog nights with drug fights just made me get my mind right\n",
      "But times I just slap on ya block and leave me bleedin'\n",
      "It's hard for me to run cause they might catch me when I'm sleepin'\n",
      "But I'm peepin' out the go round, wipe the sweat from my forehead\n",
      "Take a swig of the beer and reminice about what my boy said\n",
      "He told me keep my head up, don't get filled with all this drama\n",
      "Havin' talks with my diary cause I can't have talks with my mama\n",
      "And still feel my eyes fold I start to be the best\n",
      "Sometimes I cock my nine and wanna get it of my chest\n",
      "But I guess I'll just take it one day at a time\n",
      "But until a nigga die I got these feelings on my mind\n",
      "Fuckin' so much pain\n",
      "\n",
      "[Chorus to fade\n"
     ]
    }
   ],
   "source": [
    "print(df_en['SName'].iloc[65])\n",
    "print(df_en['Lyric'].iloc[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.026852738),\n",
       " (1, 0.5015815),\n",
       " (2, 0.075799875),\n",
       " (3, 0.10675562),\n",
       " (6, 0.023035636),\n",
       " (7, 0.019402707),\n",
       " (11, 0.13886228),\n",
       " (12, 0.090286404)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_topic_distribution[2881]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run This Town\n",
      "(Rihanna)\n",
      "Feel it coming in the air\n",
      "Hear the screams from everywhere\n",
      "I'm addicted to the the thrill\n",
      "It's a dangerous love affair\n",
      "Can't be scaring nickels down\n",
      "Got a problem, tell me now\n",
      "Only thing that's on my mind\n",
      "Is who gon' run this town tonight\n",
      "Is who gon' run this town tonight\n",
      "We gon' run this town\n",
      "\n",
      "(Jay-Z)\n",
      "We are, yeah, I said it, we are\n",
      "This is Roc Nation, pledge your allegiance\n",
      "Get y'all fatigues on, all black everything\n",
      "Black cards, black cars, all black everything\n",
      "And our girls are blackbirds, riding with they Dillingers\n",
      "I get more in-depth if you boys really real enough\n",
      "This is La Familia, I'll explain later\n",
      "But for now, let me get back to this paper\n",
      "I'm a couple bands down and I'm tryna get back\n",
      "I gave Doug a grip, I lost a flip for five stacks\n",
      "Yeah, I'm talking five comma six zeroes dot zero, here girl\n",
      "Back to running circles 'round niggas, now we squared up\n",
      "Hold up\n",
      "\n",
      "(Rihanna)\n",
      "Life's a game but it's not fair\n",
      "I break the rules so I don't care\n",
      "So I keep doing my own thing\n",
      "Walking tall against the rain\n",
      "Victory's within the mile\n",
      "Almost there, don't give up now\n",
      "Only thing that's on my mind\n",
      "Is who gon' run this town tonight\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "Who gon' run this town tonight?\n",
      "\n",
      "(Jay-Z)\n",
      "We are, yeah, I said it, we are\n",
      "You can call me Caesar, in a dark Caesar\n",
      "Please follow the leader, so Eric B. we are\n",
      "Microphone fiend, it's the return of the God, peace, God\n",
      "And ain't nobody fresher\n",
      "I'm in Maison, uh, Martin Margiela\n",
      "On the table, screaming f*** the other side, they jealous\n",
      "We got a bankhead full of broads,\n",
      "they got a table full of fellas\n",
      "(Ewww) And they ain't spendin' no cake\n",
      "They should throw they hand in,\n",
      "'cause they ain't got no spades\n",
      "(Ewww) My whole team got dough\n",
      "So my bankhead is lookin' like Millionaires' Row\n",
      "(Ewww)\n",
      "\n",
      "(Rihanna)\n",
      "Life's a game but it's not fair\n",
      "I break the rules so I don't care\n",
      "So I keep doing my own thing\n",
      "Walking tall against the rain\n",
      "Victory's within the mile\n",
      "Almost there, don't give up now\n",
      "Only thing that's on my mind\n",
      "Is who gon' run this town tonight\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "Who gon' run this town tonight?\n",
      "\n",
      "(Kanye West)\n",
      "It's crazy how you can go from being Joe Blow\n",
      "To everybody on your dick, no homo\n",
      "I bought my whole family whips, no Volvos\n",
      "Next time I'm in church, please no photos\n",
      "Police escorts, everybody passports\n",
      "This the life that everybody ask for\n",
      "This a fast life, we are on a crash course\n",
      "What you think I rap for?\n",
      "To push a fuckin' Rav 4?\n",
      "But I know that if I stay stunting\n",
      "All these girls only gon' want one thing\n",
      "I could spend my whole life good will hunting\n",
      "Only good gon' come is it's good when I'm coming\n",
      "She got an ass that'll swallow up her G-string\n",
      "And up top, uh, two bee stings\n",
      "And I'm beasting, off the re-sling\n",
      "And my nigga just made it out the precinct\n",
      "We give a damn about the drama that you do bring\n",
      "I'm just tryna change the color on your mood ring\n",
      "Reebok, baby, you need to try some new things\n",
      "Have you ever had shoes without shoestrings?\n",
      "What's that, Ye? Baby, these heels\n",
      "Is that a May-what? Baby, these wheels\n",
      "You trippin' when you ain't sippin', have a refill\n",
      "You feelin' like you run it, huh?\n",
      "Now you know how we feel\n",
      "\n",
      "(Jay-Z)\n",
      "Wha'sup?\n",
      "\n",
      "(Rihanna)\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "Hey-ey-ey-ey-ey-ey, ey-ey-ey-ey\n",
      "\n",
      "(Jay-Z)\n",
      "Wha's up?\n",
      "\n",
      "(Rihanna)\n",
      "Hey-ey-ey-ey-ey-yeah, ey-ey-ey-yeah\n",
      "Hey-ey-ey-ey-ey-yeah\n",
      "We gon' run this town tonight\n",
      "\n",
      "(Jay-Z)\n",
      "Wha's up?\n"
     ]
    }
   ],
   "source": [
    "print(df_en['SName'].iloc[2881])\n",
    "print(df_en['Lyric'].iloc[2881])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf22c5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time flies by when the night is young\n",
      "Daylight shines on an undisclosed location,\n",
      "location\n",
      "Bloodshot eyes looking for the sun\n",
      "Paradise delivered and\n",
      "we call it a vacation, vacation\n",
      "\n",
      "You're painting me a dream that I\n",
      "Wanna belong, yeah, wanna belong, yeah\n",
      "\n",
      "Over the hills and far away\n",
      "A million miles from L.A\n",
      "Just anywhere away with you\n",
      "I know we've got to get away\n",
      "Someplace where no one knows\n",
      "our name\n",
      "We'll find the start to something new\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "\n",
      "Fun, little less fun\n",
      "Little less, over, over,\n",
      "over, over, me\n",
      "Oh, fun, little less fun\n",
      "Little less, over, over,\n",
      "over, over, me\n",
      "\n",
      "Truth comes out when we're blacking out\n",
      "Looking for connection\n",
      "in a crowd of empty faces,\n",
      "empty faces\n",
      "Your secrets are the only\n",
      "thing I'm craving now\n",
      "The good, and the bad, let me in\n",
      "'Cause I can take it, I can take it\n",
      "\n",
      "You're painting me a dream that I\n",
      "Wanna belong, yeah, wanna belong, yeah\n",
      "\n",
      "Over the hills and far away\n",
      "A million miles from L.A\n",
      "Just anywhere away with you\n",
      "I know we've got to get away\n",
      "Someplace where no one\n",
      "knows our name\n",
      "We'll find the start to something new\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "\n",
      "Fun, little less fun\n",
      "Little less, over, over,\n",
      "over, over, me\n",
      "Oh, fun, little less fun\n",
      "Little less, over, over,\n",
      "over, over, me\n",
      "\n",
      "Take me anywhere\n",
      "Oh, anywhere\n",
      "Anywhere away with you\n",
      "Take me anywhere\n",
      "\n",
      "Over the hills and far away\n",
      "A million miles from L.A\n",
      "Just anywhere away with you\n",
      "I know we've got to get away\n",
      "Someplace where no one knows our name\n",
      "We'll find the start to something new\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "Just take me anywhere,\n",
      "take me anywhere\n",
      "Anywhere away with you\n",
      "\n",
      "Fun, little less fun\n",
      "Little less, over, over,\n",
      "over, over, me\n",
      "Oh\n"
     ]
    }
   ],
   "source": [
    "print(df_en['Lyric'].iloc[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b6f68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't survive\n",
      "Living without you baby\n",
      "Deep in the night\n",
      "I feel so cold inside\n",
      "\n",
      "We used to say\n",
      "\"We'll be as one forever\"\n",
      "No matter where you are\n",
      "I'll be loving you\n",
      "\n",
      "So far away\n",
      "You're so far away from me\n",
      "I can't live without you baby\n",
      "So far away\n",
      "You're so far away from me\n"
     ]
    }
   ],
   "source": [
    "print(df_en['Lyric'].iloc[1427])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c52928e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score for Song 1: 0.171\n",
      "Sentiment Score for Song 2: 0.378\n",
      "Cosine Similarity between Lyrics: 0.17731845777226657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\trevo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(lyrics):\n",
    "    sentiment = analyzer.polarity_scores(lyrics)\n",
    "    return sentiment['neg']\n",
    "\n",
    "sentiment1 = get_sentiment_score(lemmatized_corpus[496])\n",
    "sentiment2 = get_sentiment_score(lemmatized_corpus[274])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "lyrics_matrix = tfidf_vectorizer.fit_transform([lemmatized_corpus[496], lemmatized_corpus[274]])\n",
    "lyrics_similarity = cosine_similarity(lyrics_matrix)\n",
    "\n",
    "print(\"Sentiment Score for Song 1:\", sentiment1)\n",
    "print(\"Sentiment Score for Song 2:\", sentiment2)\n",
    "print(\"Cosine Similarity between Lyrics:\", lyrics_similarity[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "# Tokenize\n",
    "# Lowercase\n",
    "# punctuation removal\n",
    "# number removal\n",
    "# Stop word removal\n",
    "\n",
    "## Additional Preprocessing\n",
    "# Lemmatization and or stemming\n",
    "# Analysis word frequency determine if we need to remove high frequency low value words\n",
    "# Stop word removal\n",
    "\n",
    "## Data Check\n",
    "# Explore word stats\n",
    "# Everything is clean\n",
    "\n",
    "## NLP Processing\n",
    "# Sentiment Analysis\n",
    "# LDA\n",
    "# POS taging/Similarity\n",
    "\n",
    "## Front End\n",
    "# Django or Flask additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
